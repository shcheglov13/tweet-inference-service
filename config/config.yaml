# Основные настройки сервиса
service:
  name: tweet-inference-service
  version: 0.1.0
  host: localhost
  port: 8000
  debug: false

# Настройки модели
model:
  path: app/models/tweet_classification_model.joblib
  threshold: 0.5  # Пороговое значение для классификации
  version: 0.1.0

# Интеграция с tweet-features
feature_extraction:
  use_cache: false
  cache_dir: "./cache"
  device: "cuda"
  batch_size: 32
  log_level: "INFO"

# Настройки логирования
logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  file: C:\workspace\tweet-inference-service\tweet-inference-service.log
  max_bytes: 10485760  # 10 MB
  backup_count: 5
